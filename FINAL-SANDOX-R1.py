{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13125925,"sourceType":"datasetVersion","datasetId":8315057},{"sourceId":13126151,"sourceType":"datasetVersion","datasetId":8315212},{"sourceId":13171511,"sourceType":"datasetVersion","datasetId":8346547}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Sources\n\nThis dataset is a **custom compilation** created by combining data from multiple authoritative sources:\n\n- **NHA** â€“ [National Health Authority of India](https://nha.gov.in/)\n- **NFHS** â€“ [National Family Health Survey](http://rchiips.org/nfhs/)\n- **PMJAY** â€“ [Pradhan Mantri Jan Arogya Yojana (Ayushman Bharat)](https://pmjay.gov.in/)\n- **MoHFW** â€“ [Ministry of Health and Family Welfare](https://main.mohfw.gov.in/)\n- **MoSPI** â€“ [Ministry of Statistics and Programme Implementation](http://mospi.gov.in/)\n- **IRDAI** â€“ [Insurance Regulatory and Development Authority of India](https://irdai.gov.in/)\n- **Open Government Data** â€“ [data.gov.in](https://data.gov.in/)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npolicies_df= pd.read_csv(\"/kaggle/input/government-and-private-fixed-policy/policy3.csv\")\nagents_df = pd.read_csv(\"/kaggle/input/balanced-agents/agents_50000_balanced (2).csv\")\nagents_df = agents_df.sample(n=1000, random_state=42).reset_index(drop=True)\n# Clean column names globally\nagents_df.columns = (\n    agents_df.columns\n    .str.strip()           # remove extra spaces\n    .str.lower()           # make lowercase\n    .str.replace(\" \", \"_\") # replace spaces\n    .str.replace(\"â‚¹\", \"inr\") # replace â‚¹ with inr\n    .str.replace(\"(\", \"\")  # remove brackets\n    .str.replace(\")\", \"\")\n)\n\nprint(agents_df.columns.tolist())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:12:08.826591Z","iopub.execute_input":"2025-09-25T17:12:08.827920Z","iopub.status.idle":"2025-09-25T17:12:09.135791Z","shell.execute_reply.started":"2025-09-25T17:12:08.827885Z","shell.execute_reply":"2025-09-25T17:12:09.134635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Policy Reach Prediction using Random Forest\n\nIn this step, we train a **Random Forest Regressor** to predict the **Reach** of health policies based on their attributes.  \n\n\n\n#### Dataset\n- **Source:** `policies_df`  \n- **Features (`X`):** All columns except:\n  - `Policy Name` (identifier)  \n  - `Reach` (target variable)  \n  - `Benefit Score` (kept aside for separate use)  \n- **Target (`y`):** `Reach` â†’ number of people a policy is expected to cover.  \n\n\n\n#### Workflow\n1. **Train-Test Split**  \n   - 80% training, 20% testing.  \n   - Ensures evaluation on unseen policies.  \n\n2. **Model Training**  \n   - Algorithm: `RandomForestRegressor`  \n   - Parameters: `n_estimators=200`, `random_state=42`.  \n   - Works by creating an **ensemble of decision trees** and averaging their predictions.  \n\n\n\n#### Why Random Forest Instead of Linear Regression?\n\n- **Captures Non-linearity:**  \n  Policy reach depends on complex, non-linear factors (cost, coverage, exclusions). Random Forest models these better than a straight regression line.  \n\n- **Handles Feature Interactions:**  \n  Interactions between policy attributes (e.g., cost + coverage) are learned automatically by tree splits.  \n\n- **More Robust:**  \n  Less sensitive to outliers and skewed distributions.  \n\n- **Higher Accuracy:**  \n  By averaging predictions from many trees, Random Forest reduces variance and usually outperforms simple regression models.  \n\n\n\n#### Output\n- **RMSE:** Average error magnitude (lower = better).  \n- **RÂ² Score:** Proportion of variance explained (closer to 1 = better).  ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Features and target\nX = policies_df.drop(columns=[\"Policy Name\", \"Reach\", \"Benefit Score\"])  # keep all except Policy Name and target\ny = policies_df[\"Reach\"]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Random Forest\nrf = RandomForestRegressor(n_estimators=200, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predictions\ny_pred = rf.predict(X_test)\n\n# Evaluation\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"RÂ² Score: {r2:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:14.614445Z","iopub.execute_input":"2025-09-25T16:42:14.614910Z","iopub.status.idle":"2025-09-25T16:42:16.603345Z","shell.execute_reply.started":"2025-09-25T16:42:14.614876Z","shell.execute_reply":"2025-09-25T16:42:16.601358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimportances = rf.feature_importances_\nfeat_names = X.columns\n\nplt.figure(figsize=(10,5))\nplt.barh(feat_names, importances)\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance in Predicting Reach\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:22.276131Z","iopub.execute_input":"2025-09-25T16:42:22.276606Z","iopub.status.idle":"2025-09-25T16:42:22.813319Z","shell.execute_reply.started":"2025-09-25T16:42:22.276557Z","shell.execute_reply":"2025-09-25T16:42:22.812018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Genetic Algorithm (GA) for Policy Optimization\n\nIn this step, we use a **Genetic Algorithm (GA)** to generate and evolve new health policies.  \nThe goal: **maximize policy reach while balancing budget constraints**, using the Random Forest model trained earlier to evaluate each candidate policy.  \n\n\n\n####  How it Works\n\n1. **Population Initialization**  \n   - Start with a base policy.  \n   - Create a population of ~200 candidate policies by mutating numeric fields (e.g., premium, coverage, budget) and randomizing categorical fields (e.g., provider type, exclusions).  \n\n2. **Fitness Evaluation**  \n   - Each candidate is scored using the trained Random Forest.  \n   - **Fitness = Predicted Reach â€“ (penalty Ã— Budget)**  \n   - Policies with higher reach and lower budget are fitter.  \n\n3. **Selection**  \n   - Tournament selection: pick random groups of candidates and keep the best as parents.  \n\n4. **Crossover**  \n   - Parents exchange attributes (uniform crossover) to form a child policy.  \n\n5. **Mutation & Reseeding**  \n   - Numeric values may be multiplied by random factors (e.g., premiums â†‘ or â†“).  \n   - Occasionally, a candidate is replaced with a brand-new random policy to keep diversity.  \n\n6. **Elitism**  \n   - Top 2% of best policies are carried forward unchanged to the next generation.  \n\n7. **Evolution Loop**  \n   - Repeat the above steps for 20 generations.  \n   - At the end, return the **best policies ranked by fitness**.  \n\n\n#### Why GA?  \n- Health policy design is a **combinatorial optimization problem** with both numeric and categorical features.  \n- GA allows exploration of a large, complex search space.  \n- Balances **exploration** (mutation, reseeding) and **exploitation** (elitism, crossover).  \n\n\n\n**Output:** A ranked list of optimized synthetic policies that maximize adoption potential under budget constraints.  \n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\nPOP_SIZE = 200\nN_GENERATIONS = 20\nMUTATION_RATE = 0.2\nRESEED_PROB = 0.05 \n\nFEATURES = [\"Provider Type\", \"Target Group\", \"Monthly Premium\", \"Annual Premium\",\n            \"Coverage Amount\", \"Hospital Network Size\", \"Claim Ratio\",\n            \"Exclusions\", \"Budget\", \"Statewise Spread\"]\n\nNUMERIC_MUTATE_INFO = {\n    \"Monthly Premium\": {\"min\": 10},\n    \"Annual Premium\": {\"min\": 100},\n    \"Coverage Amount\": {\"min\": 50000},\n    \"Hospital Network Size\": {\"min\": 10},\n    \"Budget\": {\"min\": 100000},\n}\n\n\ndef safe_get_rand_choice(colname):\n    \"\"\"Return a random value from policies_df unique values for categorical columns.\"\"\"\n    if colname not in policies_df.columns:\n        return None\n    vals = policies_df[colname].dropna().unique().tolist()\n    return random.choice(vals) if vals else None\n\ndef initialize_population(base_policy, pop_size=POP_SIZE):\n    \"\"\"\n    Create initial population around base_policy (base_policy: dict-like).\n    Returns a pd.DataFrame with columns in FEATURES.\n    \"\"\"\n    rows = []\n    for _ in range(pop_size):\n        candidate = dict(base_policy)  # shallow copy\n        \n        # Ensure missing keys exist\n        for feat in FEATURES:\n            if feat not in candidate:\n                val = safe_get_rand_choice(feat)\n                candidate[feat] = val if val is not None else (NUMERIC_MUTATE_INFO.get(feat, {}).get(\"min\", 0) if feat in NUMERIC_MUTATE_INFO else 0)\n        \n        # Numeric mutations around base value (wider ranges for exploration)\n        for num_feat in NUMERIC_MUTATE_INFO:\n            val = candidate.get(num_feat, 0)\n            try:\n                val = float(val)\n            except Exception:\n                if num_feat in policies_df:\n                    val = float(policies_df[num_feat].median())\n                else:\n                    val = NUMERIC_MUTATE_INFO[num_feat][\"min\"]\n            factor = np.random.uniform(0.6, 1.8)\n            mutated = int(max(NUMERIC_MUTATE_INFO[num_feat][\"min\"], val * factor))\n            candidate[num_feat] = mutated\n        \n        # Occasionally randomize categorical fields for diversity\n        for cat in [\"Provider Type\", \"Target Group\", \"Statewise Spread\", \"Exclusions\"]:\n            if random.random() < 0.15:\n                randv = safe_get_rand_choice(cat)\n                if randv is not None:\n                    candidate[cat] = randv\n        \n        rows.append({k: candidate[k] for k in FEATURES})\n    \n    pop_df = pd.DataFrame(rows, columns=FEATURES)\n    for c in NUMERIC_MUTATE_INFO:\n        if c in pop_df:\n            pop_df[c] = pd.to_numeric(pop_df[c], errors='coerce').fillna(NUMERIC_MUTATE_INFO[c][\"min\"]).astype(float)\n    return pop_df\n\ndef fitness_function(pop_df, model):\n    \"\"\"Compute Predicted Reach and Fitness (Reach - penalty * Budget).\"\"\"\n    df = pop_df.copy()\n    X = df[FEATURES].copy()\n    for col in X.columns:\n        if col in NUMERIC_MUTATE_INFO:\n            X[col] = pd.to_numeric(X[col], errors='coerce').fillna(NUMERIC_MUTATE_INFO[col][\"min\"]).astype(float)\n        else:\n            X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0).astype(float)\n    \n    preds = model.predict(X)\n    df[\"Predicted Reach\"] = preds\n    df[\"Fitness\"] = df[\"Predicted Reach\"] - 0.001 * df[\"Budget\"]\n    return df\n\ndef select_parents(pop_df):\n    \"\"\"Tournament selection: pick k random and choose best.\"\"\"\n    k = 5\n    p1 = pop_df.sample(n=k, replace=True).sort_values(\"Fitness\", ascending=False).iloc[0]\n    p2 = pop_df.sample(n=k, replace=True).sort_values(\"Fitness\", ascending=False).iloc[0]\n    return p1.to_dict(), p2.to_dict()\n\ndef crossover(parent1_dict, parent2_dict):\n    \"\"\"Uniform crossover. Returns dict child.\"\"\"\n    child = {}\n    for feat in FEATURES:\n        if random.random() < 0.5:\n            child[feat] = parent1_dict.get(feat, parent2_dict.get(feat))\n        else:\n            child[feat] = parent2_dict.get(feat, parent1_dict.get(feat))\n    return child\n\ndef mutate(child_dict, base_policy):\n    \"\"\"Mutate numeric features; occasionally reseed the child completely.\"\"\"\n    if random.random() < RESEED_PROB:\n        new_row = initialize_population(base_policy, pop_size=1).iloc[0].to_dict()\n        return new_row\n    \n    for feat in NUMERIC_MUTATE_INFO:\n        if random.random() < MUTATION_RATE:\n            cur = float(child_dict.get(feat, NUMERIC_MUTATE_INFO[feat][\"min\"]))\n            factor = np.random.uniform(0.5, 2.0)\n            newval = int(max(NUMERIC_MUTATE_INFO[feat][\"min\"], cur * factor))\n            child_dict[feat] = newval\n    \n    for cat in [\"Provider Type\", \"Target Group\", \"Statewise Spread\", \"Exclusions\"]:\n        if random.random() < 0.08:\n            randv = safe_get_rand_choice(cat)\n            if randv is not None:\n                child_dict[cat] = randv\n    \n    return {k: (float(v) if (k in NUMERIC_MUTATE_INFO) else v) for k, v in child_dict.items()}\n\ndef run_ga(base_policy, model, n_generations=N_GENERATIONS, pop_size=POP_SIZE):\n    \"\"\"Run GA to evolve policies based on fitness from RandomForest.\"\"\"\n    population = initialize_population(base_policy, pop_size)\n    \n    for gen in range(n_generations):\n        population = fitness_function(population, model)\n        new_population = []\n        \n        # elitism: keep top 2%\n        K_elite = max(1, int(0.02 * pop_size))\n        elites = population.sort_values(\"Fitness\", ascending=False).head(K_elite)\n        for _, r in elites.iterrows():\n            new_population.append(r[FEATURES].to_dict())\n        \n        while len(new_population) < pop_size:\n            parent1, parent2 = select_parents(population)\n            child = crossover(parent1, parent2)\n            child = mutate(child, base_policy)\n            new_population.append(child)\n        \n        population = pd.DataFrame(new_population, columns=FEATURES)\n    \n    evaluated = fitness_function(population, model)\n    return evaluated.sort_values(\"Fitness\", ascending=False).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:32.871126Z","iopub.execute_input":"2025-09-25T16:42:32.871473Z","iopub.status.idle":"2025-09-25T16:42:32.906036Z","shell.execute_reply.started":"2025-09-25T16:42:32.871449Z","shell.execute_reply":"2025-09-25T16:42:32.904858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_policy = policies_df.iloc[0][FEATURES].to_dict()\n\nbest_policies = run_ga(base_policy, rf)\nprint(best_policies)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:34.467671Z","iopub.execute_input":"2025-09-25T16:42:34.468000Z","iopub.status.idle":"2025-09-25T16:42:39.918421Z","shell.execute_reply.started":"2025-09-25T16:42:34.467976Z","shell.execute_reply":"2025-09-25T16:42:39.917283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸŽ¯ Extracting the Pareto Front\n\nAfter running the Genetic Algorithm, we donâ€™t just want the single â€œbestâ€ policy.  \nInstead, we want a set of **non-dominated policies** that balance two competing objectives:\n\n1. **Maximize Predicted Reach** (cover as many people as possible).  \n2. **Minimize Budget** (reduce cost).  \n\nThis is known as the **Pareto Front** in multi-objective optimization.  \n\n- A policy is **Pareto optimal** if no other policy is **better in both objectives**.  \n- The function `get_pareto_front()` filters out all dominated solutions and returns only the non-dominated set.  \n- These represent the **trade-off frontier** â€” the set of policies worth considering further.  \n\n**Output:** A smaller dataframe containing only the policies on the Pareto front.  \n","metadata":{}},{"cell_type":"code","source":"def get_pareto_front(pop_df):\n    pop = pop_df.copy()\n    pareto = []\n\n    for i, row in pop.iterrows():\n        dominated = False\n        for j, other in pop.iterrows():\n            if (other[\"Predicted Reach\"] >= row[\"Predicted Reach\"] and\n                other[\"Budget\"] <= row[\"Budget\"] and\n                (other[\"Predicted Reach\"] > row[\"Predicted Reach\"] or \n                 other[\"Budget\"] < row[\"Budget\"])):\n                dominated = True\n                break\n        if not dominated:\n            pareto.append(row)\n    \n    return pd.DataFrame(pareto)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:48.240288Z","iopub.execute_input":"2025-09-25T16:42:48.240739Z","iopub.status.idle":"2025-09-25T16:42:48.248136Z","shell.execute_reply.started":"2025-09-25T16:42:48.240611Z","shell.execute_reply":"2025-09-25T16:42:48.246685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_pareto(pop_df, pareto_df):\n    plt.figure(figsize=(8,6))\n    plt.scatter(pop_df[\"Budget\"], pop_df[\"Predicted Reach\"], alpha=0.3, label=\"All Policies\")\n    plt.scatter(pareto_df[\"Budget\"], pareto_df[\"Predicted Reach\"], color=\"red\", label=\"Pareto Front\")\n    plt.xlabel(\"Budget\")\n    plt.ylabel(\"Predicted Reach\")\n    plt.title(\"Pareto Front: Reach vs Budget\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:50.750196Z","iopub.execute_input":"2025-09-25T16:42:50.750684Z","iopub.status.idle":"2025-09-25T16:42:50.758600Z","shell.execute_reply.started":"2025-09-25T16:42:50.750657Z","shell.execute_reply":"2025-09-25T16:42:50.757009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_policy = policies_df.iloc[0][FEATURES].to_dict()\nfinal_pop = run_ga(base_policy, rf, n_generations=20, pop_size=200)\n\npareto_front = get_pareto_front(final_pop)\nprint(\"Pareto-optimal policies:\")\nprint(pareto_front[[\"Predicted Reach\", \"Budget\"]])\n\nplot_pareto(final_pop, pareto_front)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:42:52.749231Z","iopub.execute_input":"2025-09-25T16:42:52.749886Z","iopub.status.idle":"2025-09-25T16:42:59.803404Z","shell.execute_reply.started":"2025-09-25T16:42:52.749848Z","shell.execute_reply":"2025-09-25T16:42:59.800773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\npath = \"/kaggle/input/agents-income-corrected/agents_fully_corrected.csv\"  # adjust to your file path\ndf = pd.read_csv(path)\n\nprint(\"Columns in dataset:\")\nprint(df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:43:42.830483Z","iopub.execute_input":"2025-09-25T16:43:42.830996Z","iopub.status.idle":"2025-09-25T16:43:42.853091Z","shell.execute_reply.started":"2025-09-25T16:43:42.830962Z","shell.execute_reply":"2025-09-25T16:43:42.851757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nSTATE_MAP = {\n    0: \"Sikkim\",\n    1: \"Chhattisgarh\",\n    2: \"Tamil Nadu\",\n    3: \"Uttar Pradesh\",\n    4: \"Jammu & Kashmir\",\n    5: \"Karnataka\",\n    6: \"Rajasthan\",\n    7: \"Madhya Pradesh\",\n    8: \"Haryana\",\n    9: \"Maharashtra\",\n    10: \"Bihar\",\n    11: \"Assam\",\n    12: \"Kerala\",\n    13: \"Himachal Pradesh\",\n    14: \"Delhi\",\n    15: \"Punjab\",\n    16: \"Odisha\",\n    17: \"West Bengal\",\n    18: \"Jharkhand\",\n    19: \"Andhra Pradesh\",\n    20: \"Telangana\",\n    21: \"Gujarat\",\n}\n\ndef export_policy_results_json(final_pop, agents_df, trajectories, breakdowns,\n                               top_n=5, year=-1, policy_type=\"balanced\"):\n    results = {}\n\n    # --- Ensure state column is mapped back ---\n    if agents_df[\"state\"].dtype in [\"int64\", \"float64\"]:\n        agents_df[\"state\"] = agents_df[\"state\"].map(STATE_MAP).fillna(agents_df[\"state\"].astype(str))\n    else:\n        agents_df[\"state\"] = agents_df[\"state\"].astype(str)\n\n    # ---- Top policies ----\n    top_reach = final_pop.sort_values(\"Predicted Reach\", ascending=False).head(top_n)\n    top_budget = final_pop.sort_values(\"Budget\", ascending=True).head(top_n)\n    final_pop[\"Balanced\"] = final_pop[\"Predicted Reach\"] / (final_pop[\"Budget\"]+1e-9)\n    top_balanced = final_pop.sort_values(\"Balanced\", ascending=False).head(top_n)\n\n    results[\"top_policies\"] = {\n        \"reach_based\": top_reach.to_dict(orient=\"records\"),\n        \"budget_based\": top_budget.to_dict(orient=\"records\"),\n        \"balanced\": top_balanced.to_dict(orient=\"records\"),\n    }\n\n    \n    adoption_records = []\n    rejection_records = []\n\n    # pick one policy key from chosen policy type\n    available_keys = [k for k in breakdowns.keys() if policy_type in k.lower()]\n    if not available_keys:  # fallback\n        available_keys = list(breakdowns.keys())\n    example_key = available_keys[0]\n\n    breakdown_year = breakdowns[example_key][year]\n\n    for idx, agent in agents_df.iterrows():\n        state = str(agent[\"state\"])\n        reasons_dict = breakdown_year.get(state, {})\n\n        if \"Adopted\" in reasons_dict and reasons_dict[\"Adopted\"] > 0:\n            adoption_records.append({\n                \"agent_id\": int(idx),\n                \"state\": state,\n                \"reason\": \"Adopted\"\n            })\n        else:\n            # improved rejection reasons\n            if reasons_dict:\n                reasons_only = {k: v for k, v in reasons_dict.items() if k != \"Adopted\"}\n                if reasons_only:\n                    reason = max(reasons_only.items(), key=lambda x: x[1])[0]\n                else:\n                    reason = \"Rejected: Low probability\"\n            else:\n                reason = \"Rejected: Low probability\"\n\n            rejection_records.append({\n                \"agent_id\": int(idx),\n                \"state\": state,\n                \"reason\": reason\n            })\n\n    results[\"adoption\"] = {\n        \"adopted\": adoption_records,\n        \"rejected\": rejection_records\n    }\n\n    \n    state_spread = {}\n    for state in agents_df[\"state\"].unique():\n        state_agents = agents_df[agents_df[\"state\"] == state]\n        adopt_count = sum(1 for r in adoption_records if r[\"state\"] == state)\n        state_spread[state] = {\n            \"total_families\": int(len(state_agents)),\n            \"adopted\": int(adopt_count),\n            \"adoption_rate\": float(round(100 * adopt_count / max(1, len(state_agents)), 2))\n        }\n\n    results[\"statewise_spread\"] = state_spread\n\n    return json.dumps(results, indent=2, default=str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:44:30.977185Z","iopub.execute_input":"2025-09-25T16:44:30.977688Z","iopub.status.idle":"2025-09-25T16:44:30.995219Z","shell.execute_reply.started":"2025-09-25T16:44:30.977651Z","shell.execute_reply":"2025-09-25T16:44:30.993960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean column names (remove trailing spaces, unify case)\nagents_df.columns = agents_df.columns.str.strip().str.replace(\" \", \"_\")\nprint(\"Columns:\", agents_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T16:47:07.039054Z","iopub.execute_input":"2025-09-25T16:47:07.039378Z","iopub.status.idle":"2025-09-25T16:47:07.049057Z","shell.execute_reply.started":"2025-09-25T16:47:07.039357Z","shell.execute_reply":"2025-09-25T16:47:07.047528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nYEARS = 30\nTOP_N = 5\nTWEAKS = True\n\n\nAWARENESS_YEARLY_INCREASE_URBAN = 0.6\nAWARENESS_YEARLY_INCREASE_RURAL = 0.3\nINFRA_GROWTH_URBAN = 0.015\nINFRA_GROWTH_RURAL = 0.01\nINCOME_GROWTH_BASE = 1.06\nINFLATION_BASE = 1.04\n\n\nSTATE_POLLUTION = {\n    \"Delhi\": 90, \"Uttar Pradesh\": 80, \"Bihar\": 75, \"Punjab\": 70, \"Rajasthan\": 65,\n    \"Maharastra\": 55, \"Tamil Nadu\": 40, \"Kerala\": 35, \"Himachal Pradesh\": 20,\n    \"Sikkim\": 15\n}\nSTATE_SANITATION = {  # higher = better sanitation (0-100)\n    \"Delhi\": 60, \"Uttar Pradesh\": 45, \"Bihar\": 40, \"Punjab\": 65, \"Rajasthan\": 50,\n    \"Maharastra\": 70, \"Tamil Nadu\": 75, \"Kerala\": 85, \"Himachal Pradesh\": 90,\n    \"Sikkim\": 92\n}\nSTATE_OBESITY = {  # prevalence % roughly\n    \"Delhi\": 18, \"Uttar Pradesh\": 12, \"Bihar\": 10, \"Punjab\": 20, \"Rajasthan\": 14,\n    \"Maharastra\": 16, \"Tamil Nadu\": 15, \"Kerala\": 17, \"Himachal Pradesh\": 11, \"Sikkim\": 9\n}\nSTATE_DEPRESSION_RATE = {  # prevalence %\n    \"Delhi\": 8, \"Uttar Pradesh\": 10, \"Bihar\": 9, \"Punjab\": 7, \"Rajasthan\": 8,\n    \"Maharastra\": 9, \"Tamil Nadu\": 7, \"Kerala\": 6, \"Himachal Pradesh\": 5, \"Sikkim\": 4\n}\nSTATE_SUICIDE_RATE = {  # per 100k, scaled down for use (syntax: relative index)\n    \"Delhi\": 10, \"Uttar Pradesh\": 15, \"Bihar\": 14, \"Punjab\": 18, \"Rajasthan\": 12,\n    \"Maharastra\": 13, \"Tamil Nadu\": 11, \"Kerala\": 9, \"Himachal Pradesh\": 6, \"Sikkim\": 4\n}\nSTATE_CLIMATE_RISK = {  # 0-1 index: higher means more floods/droughts etc.\n    \"Delhi\": 0.2, \"Uttar Pradesh\": 0.5, \"Bihar\": 0.6, \"Punjab\": 0.4, \"Rajasthan\": 0.7,\n    \"Maharastra\": 0.5, \"Tamil Nadu\": 0.45, \"Kerala\": 0.55, \"Himachal Pradesh\": 0.2, \"Sikkim\": 0.15\n}\n\n#default\nDEFAULT_POLLUTION = 50\nDEFAULT_SANITATION = 60\nDEFAULT_OBESITY = 12\nDEFAULT_DEPRESSION = 8\nDEFAULT_SUICIDE = 10\nDEFAULT_CLIMATE = 0.4\n\n#variations that we are adding: \ndef tweak_policies(base_policy):\n    tweaks = []\n    tweaks.append((\"Baseline\", base_policy.copy()))\n    p1 = base_policy.copy(); p1[\"Annual Premium\"] = int(p1[\"Annual Premium\"] * 0.8); tweaks.append((\"Lower Premium -20%\", p1))\n    p2 = base_policy.copy(); p2[\"Annual Premium\"] = int(p2[\"Annual Premium\"] * 1.2); tweaks.append((\"Higher Premium +20%\", p2))\n    p3 = base_policy.copy(); p3[\"Exclusions\"] = 0; tweaks.append((\"No Exclusions\", p3))\n    p4 = base_policy.copy(); p4[\"Coverage Amount\"] = int(p4[\"Coverage Amount\"] * 1.5); tweaks.append((\"Higher Coverage +50%\", p4))\n    p5 = base_policy.copy(); p5[\"Target Group\"] = 0; tweaks.append((\"All Citizens\", p5))\n    return tweaks\n\ndef pick_policy_sets(final_pop, top_k=5):\n    \"\"\"\n    From all GA policies (final_pop, ~200 rows), create tweaked variants and\n    select top_k by Reach, Budget, and Balanced trade-off.\n    Returns dict with lists of policy dicts.\n    \"\"\"\n    all_variants = []\n    for idx, row in final_pop.iterrows():\n        base_policy = row.to_dict()\n        variants = tweak_policies(base_policy)\n        for label, pol in variants:\n            pol = pol.copy()\n            pol[\"variant_label\"] = f\"Policy{idx}-{label}\"\n            pol[\"Predicted Reach\"] = row.get(\"Predicted Reach\", 0)\n            pol[\"Budget\"] = row.get(\"Budget\", 1e12)  # fallback if missing\n            pol[\"Fitness\"] = row.get(\"Fitness\", 0)\n            # balanced score = Reach / Budget (scaled)\n            pol[\"BalancedScore\"] = pol[\"Predicted Reach\"] / (pol[\"Budget\"] + 1e-9)\n            all_variants.append(pol)\n\n    all_df = pd.DataFrame(all_variants)\n\n    # Sort three ways\n    top_reach = all_df.sort_values(\"Predicted Reach\", ascending=False).head(top_k)\n    top_budget = all_df.sort_values(\"Budget\", ascending=True).head(top_k)\n    top_balanced = all_df.sort_values(\"BalancedScore\", ascending=False).head(top_k)\n\n    return {\n        \"reach\": top_reach.reset_index(drop=True),\n        \"budget\": top_budget.reset_index(drop=True),\n        \"balanced\": top_balanced.reset_index(drop=True)\n    }\n\ndef infer_state_area_params(agents_df):\n    state_params = {}\n    grouped = agents_df.groupby(\"state\")\n\n    for st, g in grouped:\n        mean_income = g[\"income_inr\"].median()\n\n        # Awareness proxy: from policy_interest (categorical)\n        if \"policy_interest\" in g.columns:\n            # Count number of interests (split by comma/space)\n            awareness_vals = g[\"policy_interest\"].astype(str).apply(lambda x: len(x.split(\",\")) if x.strip() != \"\" else 0)\n            mean_awareness = awareness_vals.mean()\n        elif \"literacy\" in g.columns:\n            mean_awareness = g[\"literacy\"].mean()\n        else:\n            mean_awareness = 0.5  # fallback\n\n        income_growth = INCOME_GROWTH_BASE if mean_income > 80000 else (INCOME_GROWTH_BASE - 0.01)\n        infra_growth = INFRA_GROWTH_URBAN if g[\"location\"].mode()[0] == \"urban\" else INFRA_GROWTH_RURAL\n\n        insured_ratio = np.mean((g[\"is_insured\"] == \"Yes\") | (g[\"is_insured\"] == True))\n        subsidy_level = 0.1 if insured_ratio > 0.5 else 0.02\n\n        pollution  = STATE_POLLUTION.get(st, DEFAULT_POLLUTION)\n        sanitation = STATE_SANITATION.get(st, DEFAULT_SANITATION)\n        obesity    = STATE_OBESITY.get(st, DEFAULT_OBESITY)\n        depression = STATE_DEPRESSION_RATE.get(st, DEFAULT_DEPRESSION)\n        suicide    = STATE_SUICIDE_RATE.get(st, DEFAULT_SUICIDE)\n        climate    = STATE_CLIMATE_RISK.get(st, DEFAULT_CLIMATE)\n\n        state_params[st] = {\n            \"income_growth\": income_growth,\n            \"infra_growth\": infra_growth,\n            \"awareness_base\": mean_awareness,\n            \"subsidy_level\": subsidy_level,\n            \"pollution\": pollution,\n            \"sanitation\": sanitation,\n            \"obesity\": obesity,\n            \"depression\": depression,\n            \"suicide\": suicide,\n            \"climate_risk\": climate\n        }\n\n    area_params = {\n        \"urban\": {\n            \"awareness_growth\": AWARENESS_YEARLY_INCREASE_URBAN / 100,\n            \"infra_growth\": INFRA_GROWTH_URBAN,\n            \"income_adj\": 1.02\n        },\n        \"rural\": {\n            \"awareness_growth\": AWARENESS_YEARLY_INCREASE_RURAL / 100,\n            \"infra_growth\": INFRA_GROWTH_RURAL,\n            \"income_adj\": 0.99\n        }\n    }\n    return state_params, area_params\n\n\n\ndef evolve_agent(agent_row, year, state_params, area_params, policy, inflation_factor, gdp_growth_factor):\n    a = agent_row.copy()\n    st = a[\"state\"]\n    loc = a[\"location\"]\n\n    # Get state-level params with safe defaults\n    stp = state_params.get(st, {\n        \"income_growth\": INCOME_GROWTH_BASE,\n        \"infra_growth\": INFRA_GROWTH_RURAL,\n        # Awareness proxy: use literacy if available\n        \"awareness_base\": float(a.get(\"literacy_num\", 50.0)),\n        \"subsidy_level\": 0.01,\n        \"pollution\": DEFAULT_POLLUTION,\n        \"sanitation\": DEFAULT_SANITATION,\n        \"obesity\": DEFAULT_OBESITY,\n        \"depression\": DEFAULT_DEPRESSION,\n        \"suicide\": DEFAULT_SUICIDE,\n        \"climate_risk\": DEFAULT_CLIMATE\n    })\n\n    ar = area_params.get(loc, area_params[\"rural\"])\n\n    # --- Income evolution ---\n    income = float(a[\"income_inr\"]) * (stp[\"income_growth\"] ** year) * (\n        1 + (ar.get(\"income_adj\", 1.0) - 1) * 0.01 * year\n    )\n\n    # --- Awareness evolution ---\n    if \"policy_interest\" in a and isinstance(a[\"policy_interest\"], str):\n        # proxy = number of policy areas mentioned\n        awareness_base = len(a[\"policy_interest\"].split(\",\"))\n    else:\n        awareness_base = float(a.get(\"literacy\", 50.0))\n\n    awareness = min(\n        100.0,\n        awareness_base + (stp[\"awareness_base\"] * 0.002) * year + ar[\"awareness_growth\"] * 100 * year,\n    )\n\n    # --- Healthcare access ---\n    healthcare_accessibility_score = float(a.get(\"accessibility_score\", 1.0))\n    policy_hosp = policy.get(\"Hospital Network Size\", 1000)\n    infra_scale = (1 + stp[\"infra_growth\"]) ** year * (1 + ar[\"infra_growth\"]) ** year\n    access_score = min(\n        5.0, healthcare_accessibility_score * infra_scale + (policy_hosp / 20000.0)\n    )\n\n    # --- Financial resilience ---\n    fin_res = float(a.get(\"financial_score\", 0.3)) * (1 + 0.005 * year)\n\n    # --- Health risk ---\n    base_health_risk = float(a.get(\"health_risk\", 0.05))\n    health_score = float(a.get(\"health_score\", 50.0))\n    comorbidity_index = np.clip((100.0 - health_score) / 100.0, 0.0, 1.0)\n\n    pollution = stp[\"pollution\"]\n    sanitation = stp[\"sanitation\"]\n    obesity = stp[\"obesity\"]\n    depression_rate = stp[\"depression\"]\n    suicide_rate = stp[\"suicide\"]\n    climate_risk = stp[\"climate_risk\"]\n\n    pollution_multiplier = 1.0 + (pollution / 200.0)\n    sanitation_penalty = 1.0 + ((50 - sanitation) / 200.0) if sanitation < 50 else 1.0\n    obesity_penalty = 1.0 + (obesity / 500.0)\n    climate_penalty = 1.0 + (climate_risk * 0.25)\n    comorbidity_penalty = 1.0 + (comorbidity_index * 0.5)\n\n    health_risk = base_health_risk * pollution_multiplier * sanitation_penalty * obesity_penalty * climate_penalty * comorbidity_penalty\n    health_risk = np.clip(health_risk, 0.0, 1.0)\n\n    # --- Mental health ---\n    mental_health_index = depression_rate / 100.0\n    stress_index = suicide_rate / 100.0\n\n    # --- Propensity ---\n    propensity_boost = 0.002 * pollution + 0.5 * comorbidity_index + 0.5 * mental_health_index\n    affordability_penalty = 0.001 * pollution + 0.001 * obesity + 0.5 * climate_risk * 0.02\n\n    # --- Spending ---\n    monthly_spend = float(a.get(\"monthly_spend_(â‚¹)\", 0)) * (INFLATION_BASE ** year)\n\n    # --- Insurance adoption probability ---\n    insured_flag = 1.0 if str(a.get(\"is_insured\")).lower() == \"yes\" else 0.0\n    insured_prob = insured_flag or min(\n        1.0, stp[\"subsidy_level\"] + 0.01 * year + 0.05 * (awareness / 100)\n    )\n\n    return {\n        \"income\": income,\n        \"awareness\": awareness,\n        \"access_score\": access_score,\n        \"fin_res\": fin_res,\n        \"health_risk\": health_risk,\n        \"mental_health_index\": mental_health_index,\n        \"stress_index\": stress_index,\n        \"comorbidity_index\": comorbidity_index,\n        \"propensity_boost\": propensity_boost,\n        \"affordability_penalty\": affordability_penalty,\n        \"insured_prob\": insured_prob,\n        \"monthly_spend\": monthly_spend,\n        \"family_size\": a.get(\"family_size\", 4),\n        \"age_group\": a.get(\"age_group\"),\n        \"gender_head_of_household\": a.get(\"gender_head_of_household\"),\n        \"literacy\": a.get(\"literacy\"),\n        \"propensity\": float(a.get(\"propensity_score\", 0)),\n        \"state\": st,\n        \"location\": loc,\n    }\n\n\ndef decide_adoption(evolved_agent, policy):\n    income = evolved_agent[\"income\"]\n    awareness = evolved_agent[\"awareness\"]\n    access = evolved_agent[\"access_score\"]\n    fin_res = evolved_agent[\"fin_res\"]\n    health_risk = evolved_agent[\"health_risk\"]\n    mental_index = evolved_agent[\"mental_health_index\"]\n    comorbidity = evolved_agent[\"comorbidity_index\"]\n    propensity = evolved_agent[\"propensity\"]/100.0 + evolved_agent.get(\"propensity_boost\", 0.0)\n    family_size = evolved_agent[\"family_size\"]\n    age_group = evolved_agent[\"age_group\"]\n    gender = evolved_agent[\"gender_head_of_household\"]\n    literacy = evolved_agent[\"literacy\"]\n\n    annual_premium = float(policy[\"Annual Premium\"])\n    exclusions = int(policy[\"Exclusions\"])\n    target_group = int(policy[\"Target Group\"])\n\n    # Eligibility checks\n    if target_group == 8 and age_group != \"senior\":\n        return False, \"Not in target group\", 0.0\n    if target_group == 19 and str(gender).lower() not in [\"female\", \"f\"]:\n        return False, \"Not in target group\", 0.0\n\n    # Affordability\n    max_affordable = income * fin_res + 1\n    affordability_raw = max_affordable / (annual_premium + 1)\n    affordability = affordability_raw - evolved_agent.get(\"affordability_penalty\", 0.0)\n    affordability = float(np.clip(affordability, 0.0, 2.0))\n\n    # Lower cutoff (0.05 instead of 0.12)\n    if affordability < 0.02:\n        return False, \"Unaffordable\", 0.0\n\n    # Exclusion penalties (softer)\n    exclusion_penalty = 0.0\n    PRE_EXISTING_CODES = {4,16,19,20,26,29}\n    MATERNITY_CODES = {11,18,23}\n    COSMETIC_CODES = {0,2,8,9,21,22,24,33,34}\n    if exclusions in PRE_EXISTING_CODES and comorbidity > 0.12:\n        exclusion_penalty += 0.15  # was 0.35\n    if exclusions in MATERNITY_CODES and age_group == \"adult\" and family_size >= 3:\n        exclusion_penalty += 0.1   # was 0.25\n    if exclusions in COSMETIC_CODES:\n        exclusion_penalty += 0.02  # was 0.05\n\n    # Behavioral & mental health effects\n    literacy_boost = 0.1 if str(literacy).lower().startswith(\"lit\") else -0.03\n    insured_penalty = -0.1 if evolved_agent[\"insured_prob\"] > 0.5 else 0.0\n    mental_need_boost = 0.12 * mental_index\n    mental_navigation_penalty = -0.05 * mental_index\n\n    # Base probability (slightly reweighted)\n    base_prob = (\n        0.28 * propensity +\n        0.22 * (access / 5.0) +\n        0.25 * (awareness / 100.0) +\n        0.2 * affordability\n    )\n    base_prob = base_prob + literacy_boost + insured_penalty + mental_need_boost + mental_navigation_penalty - exclusion_penalty\n    base_prob += 0.2 * comorbidity  # stronger nudge\n\n    adoption_prob = float(np.clip(base_prob, 0.0, 0.95))\n\n    adopt = np.random.rand() < adoption_prob\n    reason = \"Adopted\" if adopt else None\n    if not adopt:\n        reasons = {\n            \"affordability\": max(0, 0.3 - affordability),\n            \"awareness\": max(0, 0.4 - (awareness/100)),\n            \"access\": max(0, 0.2 - (access/5)),\n            \"exclusion\": exclusion_penalty,\n            \"propensity\": max(0, 0.35 - propensity),\n            \"mental\": max(0, 0.1 - mental_index)\n        }\n        reason_key = max(reasons.items(), key=lambda x: x[1])[0]\n        reason_map = {\n            \"affordability\": \"Unaffordable\",\n            \"awareness\": \"Low awareness\",\n            \"access\": \"Low healthcare access\",\n            \"exclusion\": \"Unfavorable exclusions\",\n            \"propensity\": \"Low propensity\",\n            \"mental\": \"Mental health / navigation barrier\"\n        }\n        reason = reason_map.get(reason_key, \"Rejected: Low probability\")\n\n    return adopt, reason, adoption_prob\n\n\ndef simulate_top_policies_over_time(final_pop, agents_df, top_n=TOP_N, years=YEARS, tweaks=True):\n    \"\"\"\n    Runs long-term multi-state, multi-area simulation for top N policies (with tweaks).\n    Returns:\n      - trajectories: dict {policy_label: [yearly_adoption_rate]}\n      - breakdowns: nested dict {policy_label: {year: {state: {reason: count}}}}\n    \"\"\"\n    state_params, area_params = infer_state_area_params(agents_df)\n    top_policies = final_pop.head(top_n)\n    trajectories = {}\n    breakdowns = {}\n\n    for idx, row in top_policies.iterrows():\n        base_policy = row.to_dict()\n        versions = tweak_policies(base_policy) if tweaks else [(\"Baseline\", base_policy)]\n        for label, pol in versions:\n            key = f\"Policy{idx}-{label}\"\n            trajectories[key] = []\n            breakdowns[key] = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))  # year -> state -> reason -> count\n\n            for year in range(years):\n                yearly_adopts = []\n                inflation_factor = INFLATION_BASE ** year\n                for _, agent in agents_df.iterrows():\n                    evolved = evolve_agent(agent, year, state_params, area_params, pol, inflation_factor, INCOME_GROWTH_BASE)\n                    adopt, reason, prob = decide_adoption(evolved, pol)\n\n                    # âœ… Force reason = \"Adopted\" if agent adopts\n                    if adopt:\n                        reason = \"Adopted\"\n\n                    yearly_adopts.append(1 if adopt else 0)\n                    breakdowns[key][year][agent[\"state\"]][reason] += 1\n\n                adoption_rate = np.mean(yearly_adopts)\n                trajectories[key].append(adoption_rate)\n\n    return trajectories, breakdowns\n\ndef plot_trajectories(trajectories, years=YEARS, title=\"Adoption trajectories\"):\n    plt.figure(figsize=(12,6))\n    for label, traj in trajectories.items():\n        plt.plot(range(1, years+1), np.array(traj)*100, label=label)\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Adoption rate (%)\")\n    plt.title(title)\n    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_state_breakdown_for_policy(breakdowns, policy_key, year, top_k_states=6):\n    data = breakdowns[policy_key][year]\n    state_list = sorted(data.keys(), key=lambda s: sum(data[s].values()), reverse=True)[:top_k_states]\n    reasons = set()\n    for st in state_list:\n        reasons.update(data[st].keys())\n    reasons = sorted(list(reasons))\n    matrix = []\n    for st in state_list:\n        row = [data[st].get(r, 0) for r in reasons]\n        matrix.append(row)\n    matrix = np.array(matrix)\n    totals = matrix.sum(axis=1, keepdims=True)\n    totals[totals==0] = 1\n    pct = matrix / totals\n    fig, ax = plt.subplots(figsize=(10,5))\n    bottom = np.zeros(len(state_list))\n    for i, r in enumerate(reasons):\n        ax.bar(state_list, pct[:, i]*100, bottom=bottom, label=r)\n        bottom += pct[:, i]*100\n    ax.set_ylabel(\"Proportion of agents (%)\")\n    ax.set_title(f\"Rejection/adoption reasons by state for {policy_key} in year {year+1}\")\n    ax.legend(bbox_to_anchor=(1.01, 1))\n    plt.xticks(rotation=45)\n    plt.show()\n\ndef summarize_adoptions_per_policy(breakdowns, trajectories, year=-1):\n\n    results = []\n    for policy_key, yearly_data in breakdowns.items():\n        data = yearly_data[year]\n        adopted = sum(\n            count for state_reasons in data.values()\n            for reason, count in state_reasons.items()\n            if reason == \"Adopted\"\n        )\n        rejected = sum(\n            count for state_reasons in data.values()\n            for reason, count in state_reasons.items()\n            if reason != \"Adopted\"\n        )\n        adoption_rate = round(100 * adopted / (adopted + rejected + 1e-9), 2)\n        results.append({\n            \"policy_key\": policy_key,\n            \"adopted\": int(adopted),\n            \"rejected\": int(rejected),\n            \"adoption_rate\": adoption_rate\n        })\n    return results\n\n\n\ndef print_reasons_for_policy(breakdowns, policy_key, year, top_k_states=10):\n    print(f\"\\n=== Reasons for {policy_key} in Year {year+1} ===\")\n    data = breakdowns[policy_key][year]\n\n    # Overall totals across all states\n    total_reasons = defaultdict(int)\n    for st, reasons in data.items():\n        for r, count in reasons.items():\n            total_reasons[r] += count\n\n    print(\"\\n-- Overall reasons --\")\n    for reason, count in sorted(total_reasons.items(), key=lambda x: -x[1]):\n        print(f\"{reason}: {count}\")\n\n    # Top states\n    print(f\"\\n-- State-level breakdown (top {top_k_states}) --\")\n    state_totals = {st: sum(reasons.values()) for st, reasons in data.items()}\n    top_states = sorted(state_totals.items(), key=lambda x: -x[1])[:top_k_states]\n\n    for st, _ in top_states:\n        print(f\"\\n{st}:\")\n        for reason, count in sorted(data[st].items(), key=lambda x: -x[1]):\n            print(f\"   {reason}: {count}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:07:58.029078Z","iopub.execute_input":"2025-09-25T17:07:58.029422Z","iopub.status.idle":"2025-09-25T17:07:58.100961Z","shell.execute_reply.started":"2025-09-25T17:07:58.029400Z","shell.execute_reply":"2025-09-25T17:07:58.099853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert literacy column to numeric (0/100 if categorical)\nif agents_df[\"literacy\"].dtype == \"object\":\n    agents_df[\"literacy_num\"] = agents_df[\"literacy\"].map(\n        {\"Literate\": 100.0, \"Illiterate\": 0.0}\n    ).fillna(50.0)  # fallback if weird values\nelse:\n    agents_df[\"literacy_num\"] = pd.to_numeric(agents_df[\"literacy\"], errors=\"coerce\").fillna(50.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:08:01.607745Z","iopub.execute_input":"2025-09-25T17:08:01.608081Z","iopub.status.idle":"2025-09-25T17:08:01.617077Z","shell.execute_reply.started":"2025-09-25T17:08:01.608058Z","shell.execute_reply":"2025-09-25T17:08:01.615827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trajectories, breakdowns = simulate_top_policies_over_time(\n    final_pop,\n    agents_df,\n    top_n=5,\n    years=30,\n    tweaks=True\n)\n\nplot_trajectories(trajectories, years=30, title=\"Adoption trajectories for Top 5 GA Policies\")\n\nfirst_policy_key = list(trajectories.keys())[0]\nplot_state_breakdown_for_policy(\n    breakdowns,\n    policy_key=first_policy_key,\n    year=0,\n    top_k_states=6 \n)\n\nprint(\"\\nFinal year adoption rates (Year 30):\")\nfor key, traj in trajectories.items():\n    print(f\"{key}: {traj[-1]*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:12:15.638417Z","iopub.execute_input":"2025-09-25T17:12:15.638764Z","iopub.status.idle":"2025-09-25T17:15:41.772255Z","shell.execute_reply.started":"2025-09-25T17:12:15.638741Z","shell.execute_reply":"2025-09-25T17:15:41.771063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trajectories, breakdowns = simulate_top_policies_over_time(final_pop, agents_df, top_n=3, years=10)\n\n# Pick one policy and year\npolicy_key = list(trajectories.keys())[0]\nprint_reasons_for_policy(breakdowns, policy_key, year=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:15:54.076868Z","iopub.execute_input":"2025-09-25T17:15:54.077208Z","iopub.status.idle":"2025-09-25T17:16:41.152649Z","shell.execute_reply.started":"2025-09-25T17:15:54.077186Z","shell.execute_reply":"2025-09-25T17:16:41.150881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(breakdowns.keys())[:3]\nbreakdowns[\"Policy0-Baseline\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:16:41.155401Z","iopub.execute_input":"2025-09-25T17:16:41.156277Z","iopub.status.idle":"2025-09-25T17:16:41.167765Z","shell.execute_reply.started":"2025-09-25T17:16:41.156244Z","shell.execute_reply":"2025-09-25T17:16:41.166627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef export_all_policies_breakdowns(breakdowns, year=-1, filename=\"policy_breakdowns.json\"):\n    result = {}\n\n    for policy_key, yearly_data in breakdowns.items():\n        # pick the last simulated year unless a specific year index is given\n        year_idx = year if year >= 0 else max(yearly_data.keys())\n        state_data = yearly_data[year_idx]\n\n        # convert defaultdicts to normal dicts\n        state_summary = {}\n        for state, reasons in state_data.items():\n            state_summary[state] = dict(reasons)\n\n        result[policy_key] = state_summary\n\n    # dump to file\n    with open(filename, \"w\") as f:\n        json.dump(result, f, indent=2)\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:16:41.168850Z","iopub.execute_input":"2025-09-25T17:16:41.169174Z","iopub.status.idle":"2025-09-25T17:16:41.200406Z","shell.execute_reply.started":"2025-09-25T17:16:41.169151Z","shell.execute_reply":"2025-09-25T17:16:41.198788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export JSON for all top policies (last year of simulation)\nall_breakdowns_json = export_all_policies_breakdowns(breakdowns, year=-1, filename=\"all_policies_breakdowns.json\")\n\n# If you want to quickly preview:\nprint(json.dumps(all_breakdowns_json, indent=2)[:1000])  # first 1000 chars","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:16:41.202117Z","iopub.execute_input":"2025-09-25T17:16:41.202432Z","iopub.status.idle":"2025-09-25T17:16:41.243622Z","shell.execute_reply.started":"2025-09-25T17:16:41.202402Z","shell.execute_reply":"2025-09-25T17:16:41.241868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nSTATE_MAP = {\n    0: \"Sikkim\",\n    1: \"Chhattisgarh\",\n    2: \"Tamil Nadu\",\n    3: \"Uttar Pradesh\",\n    4: \"Jammu & Kashmir\",\n    5: \"Karnataka\",\n    6: \"Rajasthan\",\n    7: \"Madhya Pradesh\",\n    8: \"Haryana\",\n    9: \"Maharashtra\",\n    10: \"Bihar\",\n    11: \"Assam\",\n    12: \"Kerala\",\n    13: \"Himachal Pradesh\",\n    14: \"Delhi\",\n    15: \"Punjab\",\n    16: \"Odisha\",\n    17: \"West Bengal\",\n    18: \"Jharkhand\",\n    19: \"Andhra Pradesh\",\n    20: \"Telangana\",\n    21: \"Gujarat\",\n}\n\ndef export_policy_results_json(final_pop, agents_df, trajectories, breakdowns,\n                               top_n=5, year=-1, policy_type=\"balanced\"):\n    results = {}\n\n    if agents_df[\"state\"].dtype in [\"int64\", \"float64\"]:\n        agents_df[\"state\"] = agents_df[\"state\"].map(STATE_MAP).fillna(agents_df[\"state\"].astype(str))\n    else:\n        agents_df[\"state\"] = agents_df[\"state\"].astype(str)\n\n    #Top policies\n    top_reach = final_pop.sort_values(\"Predicted Reach\", ascending=False).head(top_n)\n    top_budget = final_pop.sort_values(\"Budget\", ascending=True).head(top_n)\n    final_pop[\"Balanced\"] = final_pop[\"Predicted Reach\"] / (final_pop[\"Budget\"]+1e-9)\n    top_balanced = final_pop.sort_values(\"Balanced\", ascending=False).head(top_n)\n\n    results[\"top_policies\"] = {\n        \"reach_based\": top_reach.to_dict(orient=\"records\"),\n        \"budget_based\": top_budget.to_dict(orient=\"records\"),\n        \"balanced\": top_balanced.to_dict(orient=\"records\"),\n    }\n\n    return json.dumps(results, indent=2, default=str)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:16:53.673944Z","iopub.execute_input":"2025-09-25T17:16:53.674264Z","iopub.status.idle":"2025-09-25T17:16:53.683914Z","shell.execute_reply.started":"2025-09-25T17:16:53.674242Z","shell.execute_reply":"2025-09-25T17:16:53.682868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json_output = export_policy_results_json(\n    final_pop, agents_df, trajectories, breakdowns, top_n=5, year=-1\n)\n\nprint(json_output)\n\nwith open(\"policy_results.json\", \"w\") as f:\n    f.write(json_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:16:55.282009Z","iopub.execute_input":"2025-09-25T17:16:55.282839Z","iopub.status.idle":"2025-09-25T17:16:55.300991Z","shell.execute_reply.started":"2025-09-25T17:16:55.282795Z","shell.execute_reply":"2025-09-25T17:16:55.299748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decide_adoption_old(evolved_agent, policy):\n    income = evolved_agent[\"income\"]\n    awareness = evolved_agent[\"awareness\"]\n    access = evolved_agent[\"access_score\"]\n    fin_res = evolved_agent[\"fin_res\"]\n    health_risk = evolved_agent[\"health_risk\"]\n    mental_index = evolved_agent[\"mental_health_index\"]\n    stress_index = evolved_agent[\"stress_index\"]\n    comorbidity = evolved_agent[\"comorbidity_index\"]\n    propensity = evolved_agent[\"propensity\"]/100.0 + evolved_agent.get(\"propensity_boost\", 0.0)\n    family_size = evolved_agent[\"family_size\"]\n    age_group = evolved_agent[\"age_group\"]\n    gender = evolved_agent[\"gender_head_of_household\"]\n    literacy = evolved_agent[\"literacy\"]\n\n    annual_premium = float(policy[\"Annual Premium\"])\n    exclusions = int(policy[\"Exclusions\"])\n    target_group = int(policy[\"Target Group\"])\n\n    # Eligibility checks\n    if target_group == 8 and age_group != \"senior\":\n        return False, \"Not in target group\", 0.0\n    if target_group == 19 and str(gender).lower() not in [\"female\", \"f\"]:\n        return False, \"Not in target group\", 0.0\n\n    # Affordability\n    max_affordable = income * fin_res + 1\n    affordability_raw = (max_affordable / (annual_premium + 1))\n    affordability = affordability_raw - evolved_agent.get(\"affordability_penalty\", 0.0)\n    affordability = float(np.clip(affordability, 0.0, 2.0))\n\n    if affordability < 0.12:  # stricter cutoff\n        return False, \"Unaffordable\", 0.0\n\n    # Base probability (simplified)\n    base_prob = (\n        0.3 * (propensity) +\n        0.22 * (access / 5.0) +\n        0.2 * (awareness / 100.0) +\n        0.18 * affordability\n    )\n\n    adoption_prob = float(np.clip(base_prob, 0.0, 0.95))\n    adopt = np.random.rand() < adoption_prob\n\n    return adopt, (\"Adopted\" if adopt else \"Rejected: Low probability\"), adoption_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:17:06.139455Z","iopub.execute_input":"2025-09-25T17:17:06.140872Z","iopub.status.idle":"2025-09-25T17:17:06.154290Z","shell.execute_reply.started":"2025-09-25T17:17:06.140837Z","shell.execute_reply":"2025-09-25T17:17:06.152796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef compare_lowprob_to_adopt_with_states(agents_df, policy, state_params, area_params, years=1):\n    shifts = 0\n    details = []\n    statewise_counts = {st: 0 for st in agents_df[\"state\"].unique()}  # initialize with 0 for all states\n\n    for _, agent in agents_df.iterrows():\n        for year in range(years):\n            evolved = evolve_agent(agent, year, state_params, area_params, policy, 1.0, INCOME_GROWTH_BASE)\n\n            # old adoption decision\n            adopt_old, reason_old, prob_old = decide_adoption_old(evolved, policy)\n            # new adoption decision\n            adopt_new, reason_new, prob_new = decide_adoption(evolved, policy)\n\n            if reason_old == \"Rejected: Low probability\" and adopt_new:\n                shifts += 1\n                statewise_counts[evolved[\"state\"]] += 1\n                details.append({\n                    \"agent_id\": int(agent.get(\"id\", -1)),\n                    \"state\": evolved[\"state\"],\n                    \"old_reason\": reason_old,\n                    \"new_status\": \"Adopted\",\n                    \"old_prob\": round(prob_old, 3),\n                    \"new_prob\": round(prob_new, 3)\n                })\n\n    # package everything into a dict\n    results = {\n        \"total_shifts\": shifts,\n        \"examples\": details[:10],  # show first 10 examples\n        \"statewise_shifts\": statewise_counts\n    }\n\n    # export JSON string\n    return json.dumps(results, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:17:08.642941Z","iopub.execute_input":"2025-09-25T17:17:08.643267Z","iopub.status.idle":"2025-09-25T17:17:08.653343Z","shell.execute_reply.started":"2025-09-25T17:17:08.643244Z","shell.execute_reply":"2025-09-25T17:17:08.652142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"policy = final_pop.iloc[0].to_dict()\nstate_params, area_params = infer_state_area_params(agents_df)\n\njson_output = compare_lowprob_to_adopt_with_states(agents_df, policy, state_params, area_params, years=1)\n\nprint(json_output)\n\nwith open(\"policy_shift_report.json\", \"w\") as f:\n    f.write(json_output)\n\nprint(\"âœ… JSON report saved as policy_shift_report.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:17:09.559763Z","iopub.execute_input":"2025-09-25T17:17:09.560102Z","iopub.status.idle":"2025-09-25T17:17:10.013003Z","shell.execute_reply.started":"2025-09-25T17:17:09.560079Z","shell.execute_reply":"2025-09-25T17:17:10.011770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"real_policy = {\n    \"Provider Type\": 0,          \n    \"Target Group\": 0,            \n    \"Monthly Premium\": 0,        \n    \"Annual Premium\": 0,\n    \"Coverage Amount\": 500000,    \n    \"Hospital Network Size\": 25000,\n    \"Claim Ratio\": 85,\n    \"Exclusions\": 0,\n    \"Budget\": 6400000000000,      \n    \"Predicted Reach\": 0,        \n    \"Fitness\": 0               \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:17:32.017507Z","iopub.execute_input":"2025-09-25T17:17:32.017866Z","iopub.status.idle":"2025-09-25T17:17:32.026747Z","shell.execute_reply.started":"2025-09-25T17:17:32.017843Z","shell.execute_reply":"2025-09-25T17:17:32.024462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This PMJAY policy was targeted tor each around 500 million people (5 crore) where as after 10 years it reached around 320-340 million people - Government IRDAI reports (approximately 65-75%)","metadata":{}},{"cell_type":"code","source":"trajectories, breakdowns = simulate_top_policies_over_time(\n    pd.DataFrame([real_policy]), agents_df, top_n=1, years=10, tweaks=False\n)\n\n# Plot adoption trajectory\nplot_trajectories(trajectories, years=10, title=\"Real Policy Adoption (Validation)\")\n\n# Check breakdown in year 1\npolicy_key = list(breakdowns.keys())[0]\nprint(json.dumps(breakdowns[policy_key][0], indent=2, default=str))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T17:17:35.436542Z","iopub.execute_input":"2025-09-25T17:17:35.436974Z","iopub.status.idle":"2025-09-25T17:17:38.426711Z","shell.execute_reply.started":"2025-09-25T17:17:35.436949Z","shell.execute_reply":"2025-09-25T17:17:38.425633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_policy_model.py\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom joblib import dump\n\n# ==== CHANGE THIS TO YOUR CSV FILE ====\ncsv_path = \"/kaggle/input/government-and-private-fixed-policy/policy3.csv\"\n\n# ==== LOAD CSV ====\ndf = pd.read_csv(csv_path)\n\n# ==== CHOOSE FEATURES AND TARGET ====\ntarget = \"Reach\"\nfeature_cols = [\n    \"Provider Type\", \"Target Group\", \"Monthly Premium\", \"Annual Premium\",\n    \"Coverage Amount\", \"Hospital Network Size\", \"Claim Ratio\",\n    \"Exclusions\", \"Budget\"\n]\n\n# ==== DROP ROWS WITH MISSING VALUES (Optional) ====\ndf = df.dropna(subset=feature_cols + [target])\n\nX = df[feature_cols]\ny = df[target]\n\n# ==== TRAIN/TEST SPLIT FOR EVALUATION (Optional) ====\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# ==== TRAIN RANDOM FOREST ====\nmodel = RandomForestRegressor(n_estimators=200, random_state=42)\nmodel.fit(X_train, y_train)\n\n# ==== EVALUATE ====\npreds = model.predict(X_test)\nr2 = r2_score(y_test, preds)\nprint(f\"RÂ² score on test set: {r2:.4f}\")\n\n# ==== SAVE THE MODEL ====\ndump(model, \"rf_policy_model.joblib\")\nprint(\"Model saved to rf_policy_model.joblib âœ…\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T20:11:20.422521Z","iopub.execute_input":"2025-09-25T20:11:20.422881Z","iopub.status.idle":"2025-09-25T20:11:20.798993Z","shell.execute_reply.started":"2025-09-25T20:11:20.422856Z","shell.execute_reply":"2025-09-25T20:11:20.797929Z"}},"outputs":[{"name":"stdout","text":"RÂ² score on test set: 0.9470\nModel saved to rf_policy_model.joblib âœ…\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}